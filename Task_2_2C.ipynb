{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_2.2C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattagnew/SIT796-Reinforcement-Learning/blob/main/Task_2_2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN7G6cW5e768"
      },
      "source": [
        "# **Task 2.2C**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAiOX9DWZHfL"
      },
      "source": [
        "%%capture\n",
        "# INSTALL REQUIRED SYSTEM DEPENDENCIES\n",
        "!apt-get install -y xvfb x11-utils  \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            gym[box2d]==0.17.* \n",
        "!pip install pyglet\n",
        "!pip install ffmpeg\n",
        "! pip install pyvirtualdisplay\n",
        "!pip install Image\n",
        "!pip install gym-maze-trustycoder83"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLhSf9jP15B"
      },
      "source": [
        "%%capture\n",
        "!mkdir ./vid\n",
        "!rm ./vid/*.*"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPAWLq32DLLe"
      },
      "source": [
        "## Environment\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGCPcwYZD3X"
      },
      "source": [
        "import sys\n",
        "# import pygame\n",
        "import numpy as np\n",
        "# import math\n",
        "# import base64\n",
        "# import io\n",
        "# import IPython\n",
        "import gym\n",
        "import gym_maze\n",
        "\n",
        "# from gym.wrappers import Monitor\n",
        "# from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "# Recording filename\n",
        "video_name = \"./vid/Practical_2.mp4\"\n",
        "\n",
        "# Setup the environment for the maze\n",
        "env = gym.make(\"maze-sample-10x10-v0\")\n",
        "\n",
        "# Setup the video\n",
        "vid = None\n",
        "vid = video_recorder.VideoRecorder(env,video_name)\n",
        "\n",
        "# env = gym.wrappers.Monitor(env,'./vid',force=True)\n",
        "current_state = env.reset()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j3udI4Dbs6"
      },
      "source": [
        "## Run\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkSJE1LkdsZ"
      },
      "source": [
        "# DICTIONARY TO KEEP STATES/COORDS OF THE Q TABLE\n",
        "states_dic = {} \n",
        "count = 0\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        states_dic[i, j] = count\n",
        "        count+=1\n",
        "        \n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# NUMBER OF EPISODES\n",
        "n_episodes = 1000\n",
        "\n",
        "# MAX NUMBER OF ITERATIONS PER EPISODE\n",
        "max_iter_episode = 200\n",
        "\n",
        "# INITIALISE EPSILON TO 0.1\n",
        "epsilon_array = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
        "epsilon = 0.2\n",
        "\n",
        "# INITIALISE DISCOUNT FACTOR TO 0.1\n",
        "gamma_array = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
        "gamma = 0.99\n",
        "\n",
        "# SAVE REWARDS\n",
        "eps_rewards_array = np.zeros((len(epsilon_array), n_episodes))\n",
        "gam_rewards_array = np.zeros((len(gamma_array), n_episodes))\n",
        "opt_rewards_array = np.zeros((2, n_episodes))\n",
        "\n",
        "# ITERATION AVERAGES\n",
        "iter_verages = np.zeros((len(epsilon_array), max_iter_episode))\n",
        "\n",
        "# VIDEO FLAG\n",
        "video_on = 0"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCkBPmXqP5rW"
      },
      "source": [
        "### Optimistic vs Not Optimistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwwmMgZLukY0"
      },
      "source": [
        "# # # # # # # # # # # # # # # RUN MAIN ALGORITHM # # # # # # # # # # # # # # #\n",
        "opt_index = 0\n",
        "# OPTIMISTIC OR NOT OPTIMISTIC\n",
        "for opt in np.arange(2):\n",
        "\n",
        "    print(\"Optimistic on: \", opt)\n",
        "\n",
        "    # INITIALISE Q-TABLE TO 0\n",
        "    Q_table = np.zeros((len(states_dic),n_actions)) + opt * 5\n",
        "\n",
        "    # EPISODE REWARDS\n",
        "    opt_rewards = []\n",
        "\n",
        "    # ITERATE OVER EPISODES\n",
        "    for e in range(n_episodes):\n",
        "        \n",
        "        # NOT DONE\n",
        "        done = False\n",
        "        \n",
        "        # SUM THE REWARDS THAT THE AGENT GETS FROM THE ENVIRONMENT\n",
        "        rewards = []\n",
        "        total_episode_reward = 0\n",
        "        \n",
        "        # RESET N\n",
        "        N = 0\n",
        "\n",
        "        # CONSTANTS STEP SIZE\n",
        "        N = 10\n",
        "\n",
        "        for i in range(max_iter_episode): \n",
        "\n",
        "            alpha = 1.0/N\n",
        "\n",
        "            # ONLY CAPTURE VIDEO WHEN NOT LOOPING\n",
        "            if (video_on > 0):\n",
        "              env.unwrapped.render() \n",
        "              vid.capture_frame()\n",
        "\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "            # EXPLORATION\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample()\n",
        "\n",
        "            # EXPLOITATION  \n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # GET COORDINATES TO BE USED IN DICTIONARY\n",
        "            next_coordinate_x = int(next_state[0])\n",
        "            next_coordinate_y = int(next_state[1])\n",
        "\n",
        "            # UPDATE Q-TABLE USING THE Q-LEARNING ITERATION\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            Q_table[current_Q_table_coordinates, action] = \\\n",
        "                            Q_table[current_Q_table_coordinates, action] + alpha * (reward + gamma * max(Q_table[next_Q_table_coordinates,:]) - gamma * Q_table[current_Q_table_coordinates, action])\n",
        "                \n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "\n",
        "            # UPDATE REWARDS\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # IF THE EPISODE IS FINISHED, WE LEAVE THE FOR LOOP\n",
        "            if done:\n",
        "                break\n",
        "            current_state = next_state\n",
        "\n",
        "        opt_rewards.append(rewards)\n",
        "\n",
        "        # SHOW THE TOTAL EPISODE REWARD IF NOT LOOPING\n",
        "        if (video_on > 0):        \n",
        "          print(eps_index,\") Epsilon: \",epsilon,\". Total episode reward:\", total_episode_reward,\".\")\n",
        "        \n",
        "        # RESET THE ENVIRONMENT FOR NEXT EPISODE\n",
        "        current_state = env.reset()\n",
        "        \n",
        "        #rewards_per_episode.append(total_episode_reward)\n",
        "        opt_rewards_array[opt_index, e] = total_episode_reward\n",
        "\n",
        "    opt_index += 1\n",
        "    # SAVE VIDEO IF NOT LOOPING\n",
        "    if (video_on > 0):\n",
        "      print(\"Video successfuly saved.\")\n",
        "      vid.close()\n",
        "      vid.enabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO0aozn8P3-l"
      },
      "source": [
        "### Epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF_hFlbrfKCU"
      },
      "source": [
        "# # # # # # # # # # # # # # # RUN MAIN ALGORITHM # # # # # # # # # # # # # # #\n",
        "eps_index = 0\n",
        "# ITERATE OVER EPSILON\n",
        "for epsilon in epsilon_array:\n",
        "\n",
        "    print(\"Epsilon: \", epsilon)\n",
        "\n",
        "    # INITIALISE Q-TABLE TO 0\n",
        "    Q_table = np.zeros((len(states_dic),n_actions))\n",
        "\n",
        "    # EPISODE REWARDS\n",
        "    ep_rewards = []\n",
        "\n",
        "    # ITERATE OVER EPISODES\n",
        "    for e in range(n_episodes):\n",
        "        \n",
        "        # NOT DONE\n",
        "        done = False\n",
        "        \n",
        "        # SUM THE REWARDS THAT THE AGENT GETS FROM THE ENVIRONMENT\n",
        "        rewards = []\n",
        "        total_episode_reward = 0\n",
        "        \n",
        "        # RESET N\n",
        "        N = 0\n",
        "\n",
        "        # CONSTANTS STEP SIZE\n",
        "        N = 10\n",
        "\n",
        "        for i in range(max_iter_episode): \n",
        "\n",
        "            alpha = 1.0/N\n",
        "\n",
        "            # ONLY CAPTURE VIDEO WHEN NOT LOOPING\n",
        "            if (video_on > 0):\n",
        "              env.unwrapped.render() \n",
        "              vid.capture_frame()\n",
        "\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "            # EXPLORATION\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample()\n",
        "\n",
        "            # EXPLOITATION  \n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # GET COORDINATES TO BE USED IN DICTIONARY\n",
        "            next_coordinate_x = int(next_state[0])\n",
        "            next_coordinate_y = int(next_state[1])\n",
        "\n",
        "            # UPDATE Q-TABLE USING THE Q-LEARNING ITERATION\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            Q_table[current_Q_table_coordinates, action] = \\\n",
        "                            Q_table[current_Q_table_coordinates, action] + alpha * (reward + gamma * max(Q_table[next_Q_table_coordinates,:]) - gamma * Q_table[current_Q_table_coordinates, action])\n",
        "                \n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "\n",
        "            # UPDATE REWARDS\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # IF THE EPISODE IS FINISHED, WE LEAVE THE FOR LOOP\n",
        "            if done:\n",
        "                break\n",
        "            current_state = next_state\n",
        "\n",
        "        ep_rewards.append(rewards)\n",
        "\n",
        "        # SHOW THE TOTAL EPISODE REWARD IF NOT LOOPING\n",
        "        if (video_on > 0):        \n",
        "          print(eps_index,\") Epsilon: \",epsilon,\". Total episode reward:\", total_episode_reward,\".\")\n",
        "        \n",
        "        # RESET THE ENVIRONMENT FOR NEXT EPISODE\n",
        "        current_state = env.reset()\n",
        "        \n",
        "        #rewards_per_episode.append(total_episode_reward)\n",
        "        eps_rewards_array[eps_index, e] = total_episode_reward\n",
        "\n",
        "    eps_index += 1\n",
        "    # SAVE VIDEO IF NOT LOOPING\n",
        "    if (video_on > 0):\n",
        "      print(\"Video successfuly saved.\")\n",
        "      vid.close()\n",
        "      vid.enabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tTN0Nq2P2Nu"
      },
      "source": [
        "### Gamma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKnVLKC5gju5"
      },
      "source": [
        "# # # # # # # # # # # # # # # RUN MAIN ALGORITHM # # # # # # # # # # # # # # #\n",
        "gam_index = 0\n",
        "# ITERATE OVER GAMMA\n",
        "for gamma in gamma_array:\n",
        "\n",
        "    print(\"Gamma: \", gamma)\n",
        "\n",
        "    # INITIALISE Q-TABLE TO 0\n",
        "    Q_table = np.zeros((len(states_dic),n_actions))\n",
        "\n",
        "    # EPISODE REWARDS\n",
        "    gam_rewards = []\n",
        "\n",
        "    # ITERATE OVER EPISODES\n",
        "    for e in range(n_episodes):\n",
        "        \n",
        "        # NOT DONE\n",
        "        done = False\n",
        "        \n",
        "        # SUM THE REWARDS THAT THE AGENT GETS FROM THE ENVIRONMENT\n",
        "        rewards = []\n",
        "        total_episode_reward = 0\n",
        "        \n",
        "        # RESET N\n",
        "        N = 0\n",
        "\n",
        "        # CONSTANTS STEP SIZE\n",
        "        N = 10\n",
        "\n",
        "        for i in range(max_iter_episode): \n",
        "\n",
        "            alpha = 1.0/N\n",
        "\n",
        "            # ONLY CAPTURE VIDEO WHEN NOT LOOPING\n",
        "            if (video_on > 0):\n",
        "              env.unwrapped.render() \n",
        "              vid.capture_frame()\n",
        "\n",
        "            current_coordinate_x = int(current_state[0])\n",
        "            current_coordinate_y = int(current_state[1])\n",
        "            current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "            # EXPLORATION\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "              action = env.action_space.sample()\n",
        "\n",
        "            # EXPLOITATION  \n",
        "            else:\n",
        "              action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # GET COORDINATES TO BE USED IN DICTIONARY\n",
        "            next_coordinate_x = int(next_state[0])\n",
        "            next_coordinate_y = int(next_state[1])\n",
        "\n",
        "            # UPDATE Q-TABLE USING THE Q-LEARNING ITERATION\n",
        "            next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "            Q_table[current_Q_table_coordinates, action] = \\\n",
        "                            Q_table[current_Q_table_coordinates, action] + alpha * (reward + gamma * max(Q_table[next_Q_table_coordinates,:]) - gamma * Q_table[current_Q_table_coordinates, action])\n",
        "                \n",
        "            total_episode_reward = total_episode_reward + reward\n",
        "\n",
        "            # UPDATE REWARDS\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # IF THE EPISODE IS FINISHED, WE LEAVE THE FOR LOOP\n",
        "            if done:\n",
        "                break\n",
        "            current_state = next_state\n",
        "\n",
        "        ep_rewards.append(rewards)\n",
        "\n",
        "        # SHOW THE TOTAL EPISODE REWARD IF NOT LOOPING\n",
        "        if (video_on > 0):        \n",
        "          print(eps_index,\") Epsilon: \",epsilon,\". Total episode reward:\", total_episode_reward,\".\")\n",
        "        \n",
        "        # RESET THE ENVIRONMENT FOR NEXT EPISODE\n",
        "        current_state = env.reset()\n",
        "        \n",
        "        #rewards_per_episode.append(total_episode_reward)\n",
        "        gam_rewards_array[gam_index, e] = total_episode_reward\n",
        "\n",
        "    gam_index += 1\n",
        "    # SAVE VIDEO IF NOT LOOPING\n",
        "    if (video_on > 0):\n",
        "      print(\"Video successfuly saved.\")\n",
        "      vid.close()\n",
        "      vid.enabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9bbG5htaC8"
      },
      "source": [
        "## Plots\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_GeDUSjFT6c"
      },
      "source": [
        "def moving_average(array_in, size_of_window):\n",
        "    return np.convolve(array_in, np.ones(size_of_window), 'valid') / size_of_window"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS0vK57SLRQA"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "colours = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvGy5NXVPkrI"
      },
      "source": [
        "### Epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-TX19tDNvVt"
      },
      "source": [
        "moving_average_size = 11\n",
        "alpha_value = 0.3\n",
        "shifted_x = np.arange(int((moving_average_size-1)/2),len(moving_average(eps_rewards_array[0,:],moving_average_size))+int((moving_average_size-1)/2))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "ax2 = plt.gca()\n",
        "\n",
        "eps_0 = plt.plot(shifted_x,moving_average(eps_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "eps_1 = plt.plot(shifted_x,moving_average(eps_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "eps_2 = plt.plot(shifted_x,moving_average(eps_rewards_array[2,:],moving_average_size),c=colours[2])\n",
        "eps_5 = plt.plot(shifted_x,moving_average(eps_rewards_array[3,:],moving_average_size),c=colours[3])\n",
        "eps_10 = plt.plot(shifted_x,moving_average(eps_rewards_array[4,:],moving_average_size),c=colours[4])\n",
        "eps_20 = plt.plot(shifted_x,moving_average(eps_rewards_array[5,:],moving_average_size),c=colours[5])\n",
        "eps_50 = plt.plot(shifted_x,moving_average(eps_rewards_array[6,:],moving_average_size),c=colours[6])\n",
        "\n",
        "plt.plot(eps_rewards_array[0,:],c=colours[0],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[1,:],c=colours[1],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[2,:],c=colours[2],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[3,:],c=colours[3],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[4,:],c=colours[4],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[5,:],c=colours[5],alpha=alpha_value)\n",
        "plt.plot(eps_rewards_array[6,:],c=colours[6],alpha=alpha_value)\n",
        "\n",
        "axins2 = ax2.inset_axes([0.15, 0.1, 0.7, 0.5])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[2,:],moving_average_size),c=colours[2])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[3,:],moving_average_size),c=colours[3])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[4,:],moving_average_size),c=colours[4])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[5,:],moving_average_size),c=colours[5])\n",
        "axins2.plot(shifted_x,moving_average(eps_rewards_array[6,:],moving_average_size),c=colours[6])\n",
        "axins2.plot(eps_rewards_array[0,:],c=colours[0],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[1,:],c=colours[1],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[2,:],c=colours[2],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[3,:],c=colours[3],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[4,:],c=colours[4],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[5,:],c=colours[5],alpha=alpha_value/2)\n",
        "axins2.plot(eps_rewards_array[6,:],c=colours[6],alpha=alpha_value/2)\n",
        "axins2.set_xlim(0, 50)\n",
        "axins2.set_ylim(0., 1)\n",
        "ax2.indicate_inset_zoom(axins2)\n",
        "\n",
        "plt.legend((eps_0, eps_1, eps_2, eps_5, eps_10, eps_20, eps_50), labels=(r'$\\epsilon = 0.0$',r'$\\epsilon = 0.01$',r'$\\epsilon = 0.02$',r'$\\epsilon = 0.05$',r'$\\epsilon = 0.1$',r'$\\epsilon = 0.2$',r'$\\epsilon = 0.5$'), loc=\"best\")\n",
        "\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZogBytaQO_W"
      },
      "source": [
        "### Gamma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgX_iWIGQOZn"
      },
      "source": [
        "moving_average_size = 11\n",
        "alpha_value = 0.3\n",
        "shifted_x = np.arange(int((moving_average_size-1)/2),len(moving_average(gam_rewards_array[0,:],moving_average_size))+int((moving_average_size-1)/2))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "ax3 = plt.gca()\n",
        "\n",
        "gam_5 = plt.plot(shifted_x,moving_average(gam_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "gam_8 = plt.plot(shifted_x,moving_average(gam_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "gam_9 = plt.plot(shifted_x,moving_average(gam_rewards_array[2,:],moving_average_size),c=colours[2])\n",
        "gam_95 = plt.plot(shifted_x,moving_average(gam_rewards_array[3,:],moving_average_size),c=colours[3])\n",
        "gam_99 = plt.plot(shifted_x,moving_average(gam_rewards_array[4,:],moving_average_size),c=colours[4])\n",
        "\n",
        "plt.plot(gam_rewards_array[0,:],c=colours[0],alpha=alpha_value)\n",
        "plt.plot(gam_rewards_array[1,:],c=colours[1],alpha=alpha_value)\n",
        "plt.plot(gam_rewards_array[2,:],c=colours[2],alpha=alpha_value)\n",
        "plt.plot(gam_rewards_array[3,:],c=colours[3],alpha=alpha_value)\n",
        "plt.plot(gam_rewards_array[4,:],c=colours[4],alpha=alpha_value)\n",
        "\n",
        "axins3 = ax3.inset_axes([0.15, 0.1, 0.7, 0.5])\n",
        "axins3.plot(shifted_x,moving_average(gam_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "axins3.plot(shifted_x,moving_average(gam_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "axins3.plot(shifted_x,moving_average(gam_rewards_array[2,:],moving_average_size),c=colours[2])\n",
        "axins3.plot(shifted_x,moving_average(gam_rewards_array[3,:],moving_average_size),c=colours[3])\n",
        "axins3.plot(shifted_x,moving_average(gam_rewards_array[4,:],moving_average_size),c=colours[4])\n",
        "axins3.plot(gam_rewards_array[0,:],c=colours[0],alpha=alpha_value/2)\n",
        "axins3.plot(gam_rewards_array[1,:],c=colours[1],alpha=alpha_value/2)\n",
        "axins3.plot(gam_rewards_array[2,:],c=colours[2],alpha=alpha_value/2)\n",
        "axins3.plot(gam_rewards_array[3,:],c=colours[3],alpha=alpha_value/2)\n",
        "axins3.plot(gam_rewards_array[4,:],c=colours[4],alpha=alpha_value/2)\n",
        "axins3.set_xlim(0, 50)\n",
        "axins3.set_ylim(0., 1)\n",
        "ax3.indicate_inset_zoom(axins3)\n",
        "\n",
        "plt.legend([gam_5, gam_8, gam_9, gam_95, gam_99], labels=[r'$\\gamma = 0.5$',r'$\\gamma = 0.8$',r'$\\gamma = 0.9$',r'$\\gamma = 0.95$',r'$\\gamma = 0.99$'], loc=\"best\")\n",
        "\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xeW-3mdPxtc"
      },
      "source": [
        "### Optimistic vs Not Optimistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj2oa2UFtgXZ"
      },
      "source": [
        "moving_average_size = 11\n",
        "alpha_value = 0.3\n",
        "shifted_x = np.arange(int((moving_average_size-1)/2),len(moving_average(opt_rewards_array[0,:],moving_average_size))+int((moving_average_size-1)/2))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "opt_0 = plt.plot(shifted_x,moving_average(opt_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "opt_1 = plt.plot(shifted_x,moving_average(opt_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "\n",
        "plt.plot(opt_rewards_array[0,:],c=colours[0],alpha=alpha_value)\n",
        "plt.plot(opt_rewards_array[1,:],c=colours[1],alpha=alpha_value)\n",
        "\n",
        "axins = ax.inset_axes([0.15, 0.1, 0.7, 0.5])\n",
        "axins.plot(shifted_x,moving_average(opt_rewards_array[0,:],moving_average_size),c=colours[0])\n",
        "axins.plot(shifted_x,moving_average(opt_rewards_array[1,:],moving_average_size),c=colours[1])\n",
        "axins.plot(opt_rewards_array[0,:],c=colours[0],alpha=alpha_value/2)\n",
        "axins.plot(opt_rewards_array[1,:],c=colours[1],alpha=alpha_value/2)\n",
        "axins.set_xlim(0, 50)\n",
        "axins.set_ylim(0., 1)\n",
        "ax.indicate_inset_zoom(axins)\n",
        "\n",
        "plt.legend((opt_0, opt_1), labels=('Not Optimistic', 'Optimistic'), loc=\"best\")\n",
        "\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0.0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}