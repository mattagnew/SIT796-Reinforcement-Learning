{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_1.2C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattagnew/SIT796-Reinforcement-Learning/blob/main/Task_1_2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4O0tc8u6mrs"
      },
      "source": [
        "# Task 1.2C "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_e4gPxjgMKb"
      },
      "source": [
        "%%capture\n",
        "# INSTALL REQUIRED SYSTEM DEPENDENCIES\n",
        "\n",
        "!apt-get install -y xvfb x11-utils \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyVirtualDisplay==2.0.* \\\n",
        "  PyOpenGL==3.1.* \\\n",
        "  PyOpenGL-accelerate==3.1.* \\\n",
        "  gym[box2d]==0.17.* \n",
        "!pip install pyglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJw2r8V_GUL"
      },
      "source": [
        "# IMPORT REQUIRED PACKAGES\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "import IPython\n",
        "import time\n",
        "from random import randint\n",
        "from random import seed\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym import spaces\n",
        "from gym.utils import seeding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRxBIkNKv6z3"
      },
      "source": [
        "## Environment\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ABBqqyZwDCy"
      },
      "source": [
        "# DEFINE ENVIRONMENT\n",
        "\n",
        "class RocketLander(gym.Env):\n",
        "  \"\"\"A rocket landing environment for OpenAI gym\"\"\"\n",
        "  \n",
        "  metadata = {'render.modes': ['human', 'rgb_array'],\n",
        "              'video.frames_per_second':30\n",
        "  }\n",
        "\n",
        "  def __init__(self, steps_per_s=1, goal_position=0, goal_velocity=[0, 0]):\n",
        "    super(RocketLander, self).__init__()\n",
        "\n",
        "    self.seed()\n",
        "\n",
        "    self.steps_per_s = steps_per_s\n",
        "    \n",
        "    #self.np_random.uniform(low=-0.6, high=-0.4)\n",
        "    \n",
        "    self.min_position = -2000\n",
        "    self.max_position = 2000\n",
        "    self.min_vel = -9999\n",
        "    self.max_vel = 9999\n",
        "    #self.goal_position = [self.np_random.uniform(low=self.max_position/2 - self.max_position/4, \n",
        "    #                                              high=self.max_position/2 + self.max_position/4), 0]\n",
        "    self.goal_position = [0,0]\n",
        "    print(self.goal_position)\n",
        "    #goal_position = [0,0]\n",
        "    #self.goal_velocity = goal_velocity\n",
        "\n",
        "    self.booster = 19.6/steps_per_s\n",
        "    self.threshold = 1\n",
        "\n",
        "    self.gravity = -9.8/steps_per_s\n",
        "    #self.burn = self.booster + self.gravity\n",
        "\n",
        "    #self.low = np.array(\n",
        "    #  [self.min_position, self.min_vel, self.gravity], dtype=np.float32\n",
        "    #) \n",
        "\n",
        "    #self.high = np.array(\n",
        "    #  [self.max_position, self.max_vel, self.burn], dtype=np.float32\n",
        "    #)\n",
        "\n",
        "    self.viewer = None\n",
        "\n",
        "    self.action_space = spaces.Discrete(2)\n",
        "    #self.observation_space = spaces.Box(\n",
        "    #    self.low, self.high, dtype=np.float32\n",
        "    #)\n",
        "    self.observation_space = spaces.Box(\n",
        "        -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
        "    )\n",
        "\n",
        "  def seed(self,seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "\n",
        "  def step(self, action):\n",
        "    #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
        "    p = self.goal_position\n",
        "    \n",
        "    position = self.state[0]\n",
        "    velocity = self.state[1] \n",
        "    acceleration = self.state[2] \n",
        "\n",
        "    action, theta = action\n",
        "    acceleration_x = action * self.booster * (np.cos(theta))\n",
        "    acceleration_y = action * self.booster * (np.sin(theta)) + self.gravity\n",
        "\n",
        "    velocity_x = velocity[0] + acceleration_x\n",
        "    velocity_y = velocity[1] + acceleration_y\n",
        "\n",
        "    position_x = (position[0]+p[0]) + velocity[0] + 0.5*acceleration_x\n",
        "    position_y = (position[1]+p[1]) + velocity[1] + 0.5*acceleration_y\n",
        "\n",
        "    acceleration = [acceleration_x, acceleration_y]\n",
        "    velocity = [velocity_x, velocity_y]\n",
        "    position = [position_x, position_y]\n",
        "\n",
        "    #position = np.clip(position, self.min_position, self.max_position)\n",
        "    \n",
        "    #print(p)\n",
        "\n",
        "    d_x = position_x - p[0]\n",
        "    d_y = position_y - p[1]\n",
        "\n",
        "    d = [d_x, d_y]\n",
        "\n",
        "    #print(d)\n",
        "\n",
        "    done = bool(\n",
        "        d_y <= 0)\n",
        "    \n",
        "    reward = -(np.linalg.norm(d) + np.linalg.norm(velocity)) - velocity_y**2\n",
        "\n",
        "    self.state = [d, velocity, acceleration]\n",
        "    #print (self.state)\n",
        "    return np.array(self.state), reward, done, {}\n",
        "\n",
        "  def reset(self):\n",
        "    #self.state = np.array([[1000, 1000], [0,0], [0, 0]])\n",
        "    self.state = np.array([[self.np_random.uniform(low=self.min_position, \n",
        "                                                  high=self.max_position) - self.goal_position[0], \n",
        "                            self.np_random.uniform(low=2500 - self.max_position/4, \n",
        "                                                  high=2500 + self.max_position/4) - self.goal_position[0]], [0,0], [0, self.gravity]])\n",
        "        \n",
        "    return np.array(self.state)\n",
        "\n",
        "  def render(self, mode='human'):\n",
        "        screen_width = 400\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.max_position - self.min_position\n",
        "        scale = screen_width / world_width\n",
        "        rocketwidth = 20\n",
        "        rocketheight = 40\n",
        "        \n",
        "        #if (self.state[2] > 0):\n",
        "        #    burner = 1\n",
        "        #else:\n",
        "        #    burner = 0\n",
        "            \n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            #xs = np.linspace(self.min_position, self.max_position, 100)\n",
        "\n",
        "            clearance = 0\n",
        "\n",
        "            l, r, t, b = -rocketwidth / 2, rocketwidth / 2, rocketheight, 0\n",
        "            rocket = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            rocket.add_attr(rendering.Transform(translation=(0, clearance)))\n",
        "            self.rockettrans = rendering.Transform()\n",
        "            rocket.add_attr(self.rockettrans)\n",
        "            self.viewer.add_geom(rocket)\n",
        "            \n",
        "            landingx1 = (self.goal_position[0] - self.min_position) * scale - rocketwidth/2\n",
        "            landingx2 = (self.goal_position[0] - self.min_position) * scale + rocketwidth/2\n",
        "            landingheight = 5\n",
        "            landpad = rendering.FilledPolygon([(landingx1, 0), (landingx1, landingheight ), (landingx2, landingheight ), (landingx2, 0)])\n",
        "            landpad.set_color(0, 0, 1)\n",
        "            self.viewer.add_geom(landpad)\n",
        "\n",
        "\n",
        "        pos = self.state[0] \n",
        "        #print(pos)\n",
        "        #print(pos)\n",
        "        #print(np.shape(pos))\n",
        "        #print(pos[0])\n",
        "        #print(np.shape(pos[0]))\n",
        "        #pos = pos[:,0]\n",
        "        self.rockettrans.set_translation(\n",
        "            (pos[0]+self.goal_position[0]-self.min_position) * scale, (pos[1]+self.goal_position[1]) * scale\n",
        "        )\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5UmZa46mhI"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkCGmzn4-lQX"
      },
      "source": [
        "# DEFINE POLICY\n",
        "import random\n",
        "\n",
        "def policy(obs, t):\n",
        "    d = obs[0,:] \n",
        "    velocity = obs[1,:] \n",
        "    acceleration = obs[2,:] \n",
        "    \n",
        "    # CURRENT POLICY : RANDOM ACTIONS\n",
        "    # The actions are\n",
        "    #    0      Don't activate booster\n",
        "    #    1      Activate booster\n",
        "\n",
        "    # Initialise action and theta\n",
        "    action = 0\n",
        "    theta = np.pi/2\n",
        "\n",
        "    # Calculate trajectory rocket is on\n",
        "    d_x = d[0]\n",
        "    d_y = d[1]\n",
        "    v_x = velocity[0]\n",
        "    v_y = velocity[1]\n",
        "\n",
        "    t = -d_y / v_y\n",
        "\n",
        "    r_x = d_x + v_x * t\n",
        "\n",
        "    # Calculate angle of velocity vector of rocket\n",
        "    theta_opp = np.arctan2(v_y,v_x) + np.pi\n",
        "\n",
        "    # Calculate angle from landing pad to rocket\n",
        "    theta_r   = np.pi - np.arctan2(d_y, d_x)\n",
        "\n",
        "    # Set limits to when to activate booster and upper limit of speed\n",
        "    limit = d_y / 20\n",
        "    v_lim = np.clip((limit / 2), 9.8, 9999)\n",
        "\n",
        "    # If trajectory of rocket is outside landing pad + limit, fire booster against angle of rocket position\n",
        "    if abs(r_x) > limit:\n",
        "      action = 1\n",
        "      theta = theta_r\n",
        "\n",
        "    # If velocity of rocket exceeds limit, fire booster in opposite direction to velocity vector\n",
        "    if np.linalg.norm(velocity) > v_lim:\n",
        "      action = 1\n",
        "      theta = theta_opp\n",
        "\n",
        "    # If rocked exceeds an altitude of 3000, cut booster\n",
        "    if d_y > 3000:\n",
        "      action = 0\n",
        "\n",
        "    return [action, theta]"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_Zme2n6poy"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBmybV76BhIb"
      },
      "source": [
        "# RUN ENVIRONMENT\n",
        "\n",
        "!rm ./vid/*.* # CLEAN UP THE VIDEO BEFORE STARTING\n",
        "\n",
        "TIME_LIMIT = 1000 # SET TIME LIMIT\n",
        "reset_flag = 0\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "env = RocketLander()\n",
        "env = Monitor(env,'./vid',force=True)\n",
        "\n",
        "o = env.reset()\n",
        "print(o)\n",
        "#print(o)\n",
        "for t in range(TIME_LIMIT):\n",
        "    \n",
        "    action = policy(o,t)            # CALL POLICY\n",
        "    o, r, d, _ = env.step(action)   # ACTION FROM POLICY USED IN ENVIRONMENT\n",
        "    print(o, r)\n",
        "\n",
        "    if d and t<TIME_LIMIT-1:\n",
        "        print(\"Task completed in\", t, \"time steps\")\n",
        "        reset_flag = 1\n",
        "        break\n",
        "else:\n",
        "    print(\"Time limit exceeded. Try again.\")\n",
        "\n",
        "if reset_flag>0:\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqSLPRY_6sen"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loG1cqK3Bj4h"
      },
      "source": [
        "# OBSERVE VIDEO\n",
        "\n",
        "for f in env.videos:\n",
        "    video = io.open(f[0], 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display.display(display.HTML(data=\"\"\"\n",
        "        <video alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        \"\"\".format(encoded.decode('ascii'))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}